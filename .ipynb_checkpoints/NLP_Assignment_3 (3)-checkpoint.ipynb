{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad93abb9",
   "metadata": {
    "id": "ad93abb9"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import random\n",
    "import pandas as pd\n",
    "from urllib import request\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import gutenberg, stopwords, wordnet\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import ngrams\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from wordcloud import wordcloud\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from yellowbrick.text import TSNEVisualizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4jtdcdupnPY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a4jtdcdupnPY",
    "outputId": "c82e65d5-2cf5-42e9-cab1-1d0a5ff88ae2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in c:\\users\\mm\\anaconda3\\envs\\dell_env\\lib\\site-packages (1.9.2)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\mm\\anaconda3\\envs\\dell_env\\lib\\site-packages (from wordcloud) (1.20.3)\n",
      "Requirement already satisfied: pillow in c:\\users\\mm\\anaconda3\\envs\\dell_env\\lib\\site-packages (from wordcloud) (9.5.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\mm\\anaconda3\\envs\\dell_env\\lib\\site-packages (from wordcloud) (3.5.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\mm\\anaconda3\\envs\\dell_env\\lib\\site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\mm\\anaconda3\\envs\\dell_env\\lib\\site-packages (from matplotlib->wordcloud) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\mm\\anaconda3\\envs\\dell_env\\lib\\site-packages (from matplotlib->wordcloud) (1.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mm\\anaconda3\\envs\\dell_env\\lib\\site-packages (from matplotlib->wordcloud) (21.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\mm\\anaconda3\\envs\\dell_env\\lib\\site-packages (from matplotlib->wordcloud) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\mm\\anaconda3\\envs\\dell_env\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: six in c:\\users\\mm\\anaconda3\\envs\\dell_env\\lib\\site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\mm\\anaconda3\\envs\\dell_env\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\mm\\anaconda3\\envs\\dell_env\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "852589b7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "852589b7",
    "outputId": "516c6973-b635-4540-f7ed-7c6b4335cf66"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\mm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\mm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\mm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4') \n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73c202a9",
   "metadata": {
    "id": "73c202a9"
   },
   "outputs": [],
   "source": [
    "pinocchio = \"https://www.gutenberg.org/files/500/500-0.txt\"\n",
    "pan = \"https://www.gutenberg.org/files/16/16-0.txt\"\n",
    "oz = \"https://www.gutenberg.org/files/55/55-0.txt\"\n",
    "alice = \"https://www.gutenberg.org/files/11/11-0.txt\"\n",
    "gulliver = \"https://www.gutenberg.org/files/829/829-0.txt\"\n",
    "books_list = [pinocchio , pan , oz , alice , gulliver]\n",
    "author_list = [\"Carlo Collodi\", \"James M. Barrie\", \"L. Frank Baum\", \"Lewis Carroll\", \"Jonathan Swift\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5fcf7a7",
   "metadata": {
    "id": "b5fcf7a7"
   },
   "outputs": [],
   "source": [
    "def get_word_net_pos(tag):\n",
    "    if tag.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    else :\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9a5cb88",
   "metadata": {
    "id": "c9a5cb88"
   },
   "outputs": [],
   "source": [
    "def read_book(url,name) :\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    response = request.urlopen(url)\n",
    "    lines = (response.read().decode('utf8')).lower()\n",
    "    book = regexp_tokenize(lines,r'([a-zA-Z]{3,})[\\,|\\s|\\.]')\n",
    "    tokens = [token for token in book if token.lower() not in stop_words]\n",
    "    books =[]\n",
    "    for i in range(200) :\n",
    "        start = random.randint(0 , len(tokens) - 100)\n",
    "        books.append([' '.join(tokens[start: start + 100])])    \n",
    "    df = pd.DataFrame(books, columns=[\"partitions\"])\n",
    "    for i in range(200):\n",
    "        sent=\"\"\n",
    "        x = nltk.pos_tag((df.partitions[i]).split())\n",
    "        for word,tag in x:\n",
    "            lemma = lemmatizer.lemmatize(word,pos=get_word_net_pos(tag))\n",
    "            sent += lemma+\" \"\n",
    "            df.partitions[i] = sent\n",
    "    df[\"author\"] = name\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c07c5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0998f239",
   "metadata": {
    "id": "0998f239"
   },
   "outputs": [],
   "source": [
    "def five_books(list_of_urls,author_list) :\n",
    "    df_full = pd.DataFrame()\n",
    "    for i,j in enumerate(list_of_urls) :\n",
    "        df = read_book(j,author_list[i])\n",
    "        df_full = df_full.append(df , ignore_index= True)\n",
    "    return df_full    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff994e7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ff994e7",
    "outputId": "678ccafa-65ea-4464-a9ab-a6b3467540ef"
   },
   "outputs": [],
   "source": [
    "df = five_books(books_list,author_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cdea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = five_books(books_list,author_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IyFOV5PkSCSt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IyFOV5PkSCSt",
    "outputId": "48601af9-f17d-4b9e-baa7-1e970084cf16"
   },
   "outputs": [],
   "source": [
    "df.author.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z_m-vhdAomcV",
   "metadata": {
    "id": "z_m-vhdAomcV"
   },
   "outputs": [],
   "source": [
    "def draw_chart(all_books,author_list):\n",
    "    for index in author_list:\n",
    "        temp=all_books[all_books[\"author\"]==index]\n",
    "        wc = wordcloud.WordCloud(background_color='black', max_words=200, \n",
    "                             max_font_size=35)\n",
    "        wc = wc.generate(str(temp))\n",
    "        fig = plt.figure(num=1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(wc, cmap=None)\n",
    "        plt.title(f'{index}')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wsrm3IOQoqPS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wsrm3IOQoqPS",
    "outputId": "39eefb78-1a9a-47c9-a612-2609eaf4e913",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "draw_chart(df,author_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lzkiErbTozs0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "lzkiErbTozs0",
    "outputId": "9c82b29b-999c-4a3b-bf8c-ff4533420ca3"
   },
   "outputs": [],
   "source": [
    "df['char_count'] = df[\"partitions\"].apply(lambda x: sum(len(word) for word in str(x).split(\" \")))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fGeEogI5o1Ph",
   "metadata": {
    "id": "fGeEogI5o1Ph"
   },
   "outputs": [],
   "source": [
    "def draw_chart_histgram(all_books,author_list):    \n",
    "    x, y = \"char_count\", \"y\"\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "    fig.suptitle(x, fontsize=12)\n",
    "    \n",
    "    for i in all_books[\"author\"].unique():\n",
    "        sns.distplot(all_books[all_books[\"author\"]==i][x], hist=True, kde=False, \n",
    "                     bins=10, hist_kws={\"alpha\":0.8}, \n",
    "                     axlabel=\"histogram\", ax=ax[0])\n",
    "        sns.distplot(all_books[all_books[\"author\"]==i][x], hist=False, kde=True, \n",
    "                     kde_kws={\"shade\":True}, axlabel=\"density\",   \n",
    "                     ax=ax[1])\n",
    "    ax[0].grid(True)\n",
    "    ax[0].legend(all_books[\"author\"].unique())\n",
    "    ax[1].grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1BUVT0FTo3o2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1BUVT0FTo3o2",
    "outputId": "4fe1b2c2-264e-441e-ac69-16105ecef5c5"
   },
   "outputs": [],
   "source": [
    "draw_chart_histgram(df,author_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7pNWcOxEF3bV",
   "metadata": {
    "id": "7pNWcOxEF3bV"
   },
   "outputs": [],
   "source": [
    "def bow(df):\n",
    "    vectorizer = CountVectorizer()\n",
    "    bow_model = vectorizer.fit(df.partitions)\n",
    "    X = bow_model.transform(df.partitions)\n",
    "    df_bow = pd.DataFrame(X.toarray(),columns = bow_model.get_feature_names_out())\n",
    "    print(df_bow)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_bow, Y, test_size = 0.20, random_state = 42)\n",
    "    return X_train, X_test, y_train, y_test, df_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ihDb87NLSU9C",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ihDb87NLSU9C",
    "outputId": "6ecf9edb-3308-4e8e-98e9-663a914dd18a"
   },
   "outputs": [],
   "source": [
    "Y = df.author\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yVi2KHhHo7Du",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yVi2KHhHo7Du",
    "outputId": "0d07f725-d0a7-4390-ee39-ac904a530280"
   },
   "outputs": [],
   "source": [
    "a = bow(df)\n",
    "tsne = TSNEVisualizer()\n",
    "tsne.fit(a[4],df[\"author\"])\n",
    "tsne.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "K29l4Lsfo-1H",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 586
    },
    "id": "K29l4Lsfo-1H",
    "outputId": "b22534b6-bdc1-4275-a3c3-c97c6c547da5"
   },
   "outputs": [],
   "source": [
    "def get_top_n_words(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(1,1)).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "common_words = get_top_n_words(df['partitions'], 20)\n",
    "#for word, freq in common_words:\n",
    "    #print(word, freq)\n",
    "df1 = pd.DataFrame(common_words, columns = ['word' , 'count'])\n",
    "df1.groupby('word').sum()['count'].sort_values(ascending=False).plot(kind='bar', title='Top 20 words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fopbFmNpNBsG",
   "metadata": {
    "id": "fopbFmNpNBsG"
   },
   "outputs": [],
   "source": [
    "def tf_idf(df):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tf_idf_model = vectorizer.fit(df.partitions)\n",
    "    tf_idf_out=tf_idf_model.transform(df.partitions)\n",
    "    feature_names = tf_idf_model.get_feature_names_out()\n",
    "    dense = tf_idf_out.todense()\n",
    "    denselist = dense.tolist()\n",
    "    df = pd.DataFrame(denselist, columns=feature_names)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df, Y, test_size = 0.20, random_state = 42)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_l_CyE0rNBu0",
   "metadata": {
    "id": "_l_CyE0rNBu0"
   },
   "outputs": [],
   "source": [
    "def n_gram(df , n):\n",
    "    count_vect = CountVectorizer(ngram_range=(n , n))\n",
    "    ngram_vectors=count_vect.fit_transform(df.partitions)\n",
    "    df = pd.DataFrame(ngram_vectors.toarray(),columns=count_vect.get_feature_names_out())\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df, Y, test_size = 0.20, random_state = 42)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gddYQeKFQHhn",
   "metadata": {
    "id": "gddYQeKFQHhn"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "df.author = encoder.fit_transform(df.author)\n",
    "Y = df.author\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yVMB_WHdUaP6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yVMB_WHdUaP6",
    "outputId": "e42ca52f-f12f-4b5d-8cb8-218da66878c1"
   },
   "outputs": [],
   "source": [
    "X_train_bow, X_test_bow, y_train_bow, y_test_bow,hh = bow(df)\n",
    "X_train_tf_idf, X_test_tf_idf, y_train_tf_idf, y_test_tf_idf = tf_idf(df)\n",
    "X_train_n_gram, X_test_n_gram, y_train_n_gram, y_test_n_gram = n_gram(df,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8-AZ_2Whe14q",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "8-AZ_2Whe14q",
    "outputId": "8b6f8054-9923-4c2b-d7cb-fd4a36e370ba"
   },
   "outputs": [],
   "source": [
    "X_train_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IzvtCfuOVdpV",
   "metadata": {
    "id": "IzvtCfuOVdpV"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "def cross_validation(model, X, y):\n",
    "    scoring = ['accuracy', 'f1']\n",
    "    scores = cross_val_score(model, X, y, cv=2)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98db956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_mx (y_test,y_pred,model_name):\n",
    "    cm = confusion_matrix(y_test,y_pred)\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    sns.heatmap(cm, annot=True,fmt='g', cmap='Blues')\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.title(f'{model_name} Confusion Matrix')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc48e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit(X_train, y_train, X_test, y_test,nb,model_name):\n",
    "    nb.fit(X_train, y_train)\n",
    "    y_pred = nb.predict(X_test)\n",
    "    confusion_mx(y_test, y_pred,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yZaHEOp_NBxS",
   "metadata": {
    "id": "yZaHEOp_NBxS"
   },
   "outputs": [],
   "source": [
    "def svm():\n",
    "    svc = SVC(kernel=\"rbf\")\n",
    "    print(cross_validation(svc,X_train_bow,y_train_bow))\n",
    "    print(cross_validation(svc,X_train_tf_idf,y_train_tf_idf))\n",
    "    print(cross_validation(svc,X_train_n_gram,y_train_n_gram))\n",
    "    model_fit(X_train_bow, y_train_bow, X_test_bow, y_test_bow, svc,\"SVM BOW\")\n",
    "    model_fit(X_train_tf_idf,y_train_tf_idf, X_test_tf_idf,  y_test_tf_idf,svc,\"SVM TF_IDF\")\n",
    "    model_fit(X_train_n_gram, y_train_n_gram, X_test_n_gram, y_test_n_gram,svc,\"SVM N_GRAM\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf4335e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LVtxiiG-Y7bU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LVtxiiG-Y7bU",
    "outputId": "7c23ceec-b4c2-4e72-dc36-30a7ee98996d"
   },
   "outputs": [],
   "source": [
    "svm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kpxqOd5mZgD5",
   "metadata": {
    "id": "kpxqOd5mZgD5"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "def naive_bayes():\n",
    "    nb = MultinomialNB()\n",
    "    print(cross_validation(nb, X_train_bow, y_train_bow))\n",
    "    print(cross_validation(nb,X_train_tf_idf,y_train_tf_idf))\n",
    "    print(cross_validation(nb,X_train_n_gram,y_train_n_gram))\n",
    "    model_fit(X_train_bow, y_train_bow,X_test_bow,y_test_bow,nb,\"Naïve Bayes BOW\")\n",
    "    model_fit(X_train_tf_idf,y_train_tf_idf, X_test_tf_idf,  y_test_tf_idf,nb,\"Naïve Bayes TF_IDF\")\n",
    "    model_fit(X_train_n_gram, y_train_n_gram, X_test_n_gram, y_test_n_gram,nb,\"Naïve Bayes N_GRAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e3d90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a74457",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def knn():\n",
    "    k = 5  # Set the value of k for KNN\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    print(cross_validation(knn, X_train_bow, y_train_bow))\n",
    "    print(cross_validation(knn,X_train_tf_idf,y_train_tf_idf))\n",
    "    print(cross_validation(knn,X_train_n_gram,y_train_n_gram))\n",
    "    model_fit(X_train_bow, y_train_bow,X_test_bow,y_test_bow,knn,\"KNN BOW\")\n",
    "    model_fit(X_train_tf_idf,y_train_tf_idf, X_test_tf_idf,  y_test_tf_idf,knn,\"KNN TF_IDF\")\n",
    "    model_fit(X_train_n_gram, y_train_n_gram, X_test_n_gram, y_test_n_gram,knn,\"KNN N_GRAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181d57e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d6d95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForest():\n",
    "    Forest_model = RandomForestClassifier(n_estimators = 200, criterion = 'entropy', random_state = 42)\n",
    "    print(cross_validation(Forest_model,X_train_bow,y_train_bow))\n",
    "    print(cross_validation(Forest_model,X_train_tf_idf,y_train_tf_idf))\n",
    "    print(cross_validation(Forest_model,X_train_n_gram,y_train_n_gram))\n",
    "    model_fit(X_train_bow, y_train_bow,X_test_bow,y_test_bow,Forest_model,\"Random Forest BOW\")\n",
    "    model_fit(X_train_tf_idf,y_train_tf_idf, X_test_tf_idf,  y_test_tf_idf,Forest_model,\"Random Forest TF_IDF\")\n",
    "    model_fit(X_train_n_gram, y_train_n_gram, X_test_n_gram, y_test_n_gram,Forest_model,\"Random Forest N_GRAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8fbd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813b9c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD():\n",
    "    sgd = SGDClassifier(random_state=42)\n",
    "    print(cross_validation(sgd,X_train_bow,y_train_bow))\n",
    "    print(cross_validation(sgd,X_train_tf_idf,y_train_tf_idf))\n",
    "    print(cross_validation(sgd,X_train_n_gram,y_train_n_gram))\n",
    "    model_fit(X_train_bow, y_train_bow,X_test_bow,y_test_bow,sgd,\"SGD BOW\")\n",
    "    model_fit(X_train_tf_idf,y_train_tf_idf, X_test_tf_idf,  y_test_tf_idf,sgd,\"SGD TF_IDF\")\n",
    "    model_fit(X_train_n_gram, y_train_n_gram, X_test_n_gram, y_test_n_gram,sgd,\"SGD N_GRAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03c4ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SGD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b190c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_m():\n",
    "    xgb_model = xgb.XGBClassifier(random_state=42)\n",
    "    print(cross_validation(xgb_model,X_train_bow,y_train_bow))\n",
    "    print(cross_validation(xgb_model,X_train_tf_idf,y_train_tf_idf))\n",
    "    print(cross_validation(xgb_model,X_train_n_gram,y_train_n_gram))\n",
    "    model_fit(X_train_bow, y_train_bow,X_test_bow,y_test_bow,xgb_model,\"XGB BOW\")\n",
    "    model_fit(X_train_tf_idf,y_train_tf_idf, X_test_tf_idf,  y_test_tf_idf,xgb_model,\"XGB TF_IDF\")\n",
    "    model_fit(X_train_n_gram, y_train_n_gram, X_test_n_gram, y_test_n_gram,xgb_model,\"XGB N_GRAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead510bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_m() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61cc3d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddde6bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567109ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c610a464",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b7e13b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e4bfa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccb35ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fbce10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb85056",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbccce4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ceecc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daebbe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00182356",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a9ba11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310eeb4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
