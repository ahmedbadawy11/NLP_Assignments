{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37b9c50d",
   "metadata": {},
   "source": [
    "## Install the Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4jtdcdupnPY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a4jtdcdupnPY",
    "outputId": "c82e65d5-2cf5-42e9-cab1-1d0a5ff88ae2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in c:\\users\\mm\\anaconda3\\envs\\dell_env\\lib\\site-packages (1.9.2)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\mm\\anaconda3\\envs\\dell_env\\lib\\site-packages (from wordcloud) (1.20.3)\n",
      "Requirement already satisfied: pillow in c:\\users\\mm\\anaconda3\\envs\\dell_env\\lib\\site-packages (from wordcloud) (9.5.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\mm\\anaconda3\\envs\\dell_env\\lib\\site-packages (from wordcloud) (3.5.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\mm\\anaconda3\\envs\\dell_env\\lib\\site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\mm\\anaconda3\\envs\\dell_env\\lib\\site-packages (from matplotlib->wordcloud) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\mm\\anaconda3\\envs\\dell_env\\lib\\site-packages (from matplotlib->wordcloud) (1.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mm\\anaconda3\\envs\\dell_env\\lib\\site-packages (from matplotlib->wordcloud) (21.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\mm\\anaconda3\\envs\\dell_env\\lib\\site-packages (from matplotlib->wordcloud) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\mm\\anaconda3\\envs\\dell_env\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: six in c:\\users\\mm\\anaconda3\\envs\\dell_env\\lib\\site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\mm\\anaconda3\\envs\\dell_env\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\mm\\anaconda3\\envs\\dell_env\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ae4e8f",
   "metadata": {},
   "source": [
    "## Import the Needed Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad93abb9",
   "metadata": {
    "id": "ad93abb9"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib import request\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import gutenberg, stopwords, wordnet\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import ngrams\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from wordcloud import wordcloud\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from yellowbrick.text import TSNEVisualizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import os\n",
    "import multiprocessing\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(multiprocessing.cpu_count())\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed698ae3",
   "metadata": {},
   "source": [
    "## Download the Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "852589b7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "852589b7",
    "outputId": "516c6973-b635-4540-f7ed-7c6b4335cf66"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\mm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\mm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\mm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4') \n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605ffd0e",
   "metadata": {},
   "source": [
    "# 1 - PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9f6ab2",
   "metadata": {},
   "source": [
    "### Function to get the Wordnet's Part of Speach for future use to lemmatize the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5fcf7a7",
   "metadata": {
    "id": "b5fcf7a7"
   },
   "outputs": [],
   "source": [
    "def get_word_net_pos(tag):\n",
    "    if tag.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    else :\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c4670a",
   "metadata": {},
   "source": [
    "## Function \"read_book\"\n",
    "\n",
    "\n",
    "* **It Takes 2 parameters the URL of the Book and the Name of the book**\n",
    "* **It Reads the Book, Decode, lower the Characters, Remove the Unnecessary characters, Remove the Stopwords, and Tokenize them**\n",
    "\n",
    "\n",
    "* **It Gets 200 Random Samples each Consists of 100 Word** \n",
    "* **Then, Using the Previous \"get_word_net_pos\" Function we lemmatize the Tokens and Label the Book with its Author name** \n",
    "\n",
    "\n",
    "* **Finally, We Return the Dataframe**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9a5cb88",
   "metadata": {
    "id": "c9a5cb88"
   },
   "outputs": [],
   "source": [
    "def read_book(url,name , word_num) :\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    response = request.urlopen(url)\n",
    "    lines = (response.read().decode('utf8')).lower()\n",
    "    book = regexp_tokenize(lines,r'([a-zA-Z]{3,})[\\,|\\s|\\.]')\n",
    "    tokens = [token for token in book if token.lower() not in stop_words]\n",
    "    books =[]\n",
    "    for i in range(200) :\n",
    "        start = random.randint(0 , len(tokens) - word_num)\n",
    "        books.append([' '.join(tokens[start: start + word_num])])    \n",
    "    df = pd.DataFrame(books, columns=[\"partitions\"])\n",
    "    for i in range(200):\n",
    "        sent=\"\"\n",
    "        x = nltk.pos_tag((df.partitions[i]).split())\n",
    "        for word,tag in x:\n",
    "            lemma = lemmatizer.lemmatize(word,pos=get_word_net_pos(tag))\n",
    "            sent += lemma+\" \"\n",
    "            df.partitions[i] = sent\n",
    "    df[\"author\"] = name\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef8e089",
   "metadata": {},
   "source": [
    "## Function \"five_books\"\n",
    "\n",
    "\n",
    "*   **It Takes the List of URLs of the Books and the Authors Lisr as parameters**\n",
    "*   **And for each URL it Reads the Book and Preprocess it as Above then Append all of the Dataframes into a Full Dataframe Contains all the Books Samples** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0998f239",
   "metadata": {
    "id": "0998f239"
   },
   "outputs": [],
   "source": [
    "def five_books(list_of_urls,author_list , word_num) :\n",
    "    df_full = pd.DataFrame()\n",
    "    for i,j in enumerate(list_of_urls) :\n",
    "        df = read_book(j,author_list[i] , word_num)\n",
    "        df_full = df_full.append(df , ignore_index= True)\n",
    "    return df_full    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40d50ee",
   "metadata": {},
   "source": [
    "# 2-VISUALIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90cf61a",
   "metadata": {},
   "source": [
    "## Function \"draw_chart\"\n",
    "* **To show the Word Denisty for all Books**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "z_m-vhdAomcV",
   "metadata": {
    "id": "z_m-vhdAomcV"
   },
   "outputs": [],
   "source": [
    "def draw_chart(all_books,author_list):\n",
    "    for index in author_list:\n",
    "        temp=all_books[all_books[\"author\"]==index]\n",
    "        wc = wordcloud.WordCloud(background_color='black', max_words=200, \n",
    "                             max_font_size=35)\n",
    "        wc = wc.generate(str(temp))\n",
    "        fig = plt.figure(num=1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(wc, cmap=None)\n",
    "        plt.title(f'{index}')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47e90c0",
   "metadata": {},
   "source": [
    "## Function \"draw_chart_histogram\"\n",
    " * **To Display the Characters' count for each Authors' Book**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fGeEogI5o1Ph",
   "metadata": {
    "id": "fGeEogI5o1Ph"
   },
   "outputs": [],
   "source": [
    "def draw_chart_histogram(all_books,author_list):    \n",
    "    x, y = \"char_count\", \"y\"\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "    fig.suptitle(x, fontsize=12)\n",
    "    \n",
    "    for i in all_books[\"author\"].unique():\n",
    "        sns.distplot(all_books[all_books[\"author\"]==i][x], hist=True, kde=False, \n",
    "                     bins=10, hist_kws={\"alpha\":0.8}, \n",
    "                     axlabel=\"histogram\", ax=ax[0])\n",
    "        sns.distplot(all_books[all_books[\"author\"]==i][x], hist=False, kde=True, \n",
    "                     kde_kws={\"shade\":True}, axlabel=\"density\",   \n",
    "                     ax=ax[1])\n",
    "    ax[0].grid(True)\n",
    "    ax[0].legend(all_books[\"author\"].unique())\n",
    "    ax[1].grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401e2244",
   "metadata": {},
   "source": [
    "## Function get_top_n_words\n",
    "* **To Display the most Repeated 20 Words**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dea2ffa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_words(corpus, n):\n",
    "    vec = CountVectorizer(ngram_range=(1,1)).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ecfb60",
   "metadata": {},
   "source": [
    "# TRANSFORMATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5744a656",
   "metadata": {},
   "source": [
    "## Function bow \n",
    " * **Convert the Text Dataframe into a Bag of Words Representation** \n",
    " * **Then, Split the BOW Dataframe into Train and Test Sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7pNWcOxEF3bV",
   "metadata": {
    "id": "7pNWcOxEF3bV"
   },
   "outputs": [],
   "source": [
    "def bow(df):\n",
    "    vectorizer = CountVectorizer()\n",
    "    bow_model = vectorizer.fit(df.partitions)\n",
    "    X = bow_model.transform(df.partitions)\n",
    "    df_bow = pd.DataFrame(X.toarray(),columns = bow_model.get_feature_names_out())\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_bow, Y,stratify=Y  , test_size = 0.20, random_state = 42)\n",
    "    return X_train, X_test, y_train, y_test, df_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45a9da0",
   "metadata": {},
   "source": [
    "## Function tf_idf\n",
    "* **Convert the Text Dataframe into a TF_IDF  Representation** \n",
    "* **Then, Split the TF_IDF Dataframe into Train and Test Sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fopbFmNpNBsG",
   "metadata": {
    "id": "fopbFmNpNBsG"
   },
   "outputs": [],
   "source": [
    "def tf_idf(df):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tf_idf_model = vectorizer.fit(df.partitions)\n",
    "    tf_idf_out=tf_idf_model.transform(df.partitions)\n",
    "    feature_names = tf_idf_model.get_feature_names_out()\n",
    "    dense = tf_idf_out.todense()\n",
    "    denselist = dense.tolist()\n",
    "    df = pd.DataFrame(denselist, columns=feature_names)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df, Y,stratify=Y  , test_size = 0.20, random_state = 42)\n",
    "    return X_train, X_test, y_train, y_test,df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b67d88",
   "metadata": {},
   "source": [
    "## Function n_gram\n",
    " * **Convert the Text Dataframe into a N_Gram Representation**\n",
    " * **Then, Split the N_Gram Dataframe into Train and Test Sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "_l_CyE0rNBu0",
   "metadata": {
    "id": "_l_CyE0rNBu0"
   },
   "outputs": [],
   "source": [
    "def n_gram(df , n):\n",
    "    count_vect = CountVectorizer(ngram_range=(n , n))\n",
    "    ngram_vectors=count_vect.fit_transform(df.partitions)\n",
    "    df = pd.DataFrame(ngram_vectors.toarray(),columns=count_vect.get_feature_names_out())\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df, Y,stratify=Y  ,test_size = 0.20, random_state = 42)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test,df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d845dae",
   "metadata": {},
   "source": [
    "## Function \"draw_TSNE\"\n",
    "* **To Display the Scatter plot for each Book To see the Similarity between these Books**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e68da5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_TSNE (df_input):\n",
    "    tsne = TSNEVisualizer()\n",
    "    tsne.fit(df_input,df[\"author\"])\n",
    "    tsne.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5957b801",
   "metadata": {},
   "source": [
    "## cross_validation\n",
    "\n",
    "\n",
    "* **this is user defined function which take estimator[fit ],x_train and y_test**\n",
    "* **then path them to cross_val_score function to Evaluate a score by cross-validation.**\n",
    "* **then return scores whice contain taining score and validation score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "IzvtCfuOVdpV",
   "metadata": {
    "id": "IzvtCfuOVdpV"
   },
   "outputs": [],
   "source": [
    "cv =2\n",
    "def cross_validation(model, X, y):\n",
    "    scores = cross_val_score(model, X, y, cv=cv )\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952d1254",
   "metadata": {},
   "source": [
    "## confusion_mx\n",
    "\n",
    "* **this is user defined function which take y_test , y_predict and model_name**\n",
    "\n",
    "* **then path them to confusion_matrix function**\n",
    "* **to draw and calculate confusion_matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d98db956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_mx (y_test,y_pred,model_name):\n",
    "    cm = confusion_matrix(y_test,y_pred)\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    sns.heatmap(cm, annot=True,fmt='g', cmap='Blues')\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.title(f'{model_name} Confusion Matrix')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a72636c",
   "metadata": {},
   "source": [
    "##  bar_plot_val\n",
    "\n",
    "\n",
    "* **this is user defined function which take results [list contain train and validation score]**\n",
    "\n",
    "* **then draw bar chart to visualize train and validation score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d028eba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_plot_val(results):\n",
    "    test_score=results['test_score']\n",
    "    train_score=results['train_score']\n",
    "    X_axis = np.arange(len(test_score))\n",
    "    plt.xticks(X_axis+3)\n",
    "    plt.bar(X_axis-0.2, test_score, 0.4, color='black', label='Validation')\n",
    "    plt.bar(X_axis+0.2, train_score, 0.4, color='red', label='Training')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621dec50",
   "metadata": {},
   "source": [
    "## model_fit\n",
    "\n",
    "* **this is user defined function which take X_train ,y_train ,X_test,y_test,model and model_name**\n",
    "\n",
    "* **then path X_train and y_train to fit function then path X_test to  predict functoin to calculate  y_pred**\n",
    "\n",
    "* **then path X_train , y_train and estimator with 10 k_fold  to cross_validate fun**\n",
    "\n",
    "* **then path the results from cross_validate to bar_plot_val function** \n",
    "\n",
    "* **then path y_test ,y_pred and model_name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8dc48e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit(X_train, y_train, X_test, y_test, model, model_name):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    results = cross_validate(estimator=model, X=X_train, y=y_train, cv=cv, return_train_score=True)\n",
    "    print(f\"Score {model_name} \",end=\"\")\n",
    "    print(cross_validation(model,X_train,y_train))\n",
    "    bar_plot_val(results)\n",
    "    confusion_mx(y_test, y_pred,model_name)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8901f4ef",
   "metadata": {},
   "source": [
    "## svm\n",
    "\n",
    "* **this is user defined function whice apply  SVM model  using \\\"rbf\\\" kernel**\n",
    "* **then calculate cross_validation for all three Transform**\n",
    "* **[ BOW, and TF-IDF, n-gram]**\n",
    "* **then calculate model_fit  for all three Transform [ BOW, and TF-IDF, n-gram]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "yZaHEOp_NBxS",
   "metadata": {
    "id": "yZaHEOp_NBxS"
   },
   "outputs": [],
   "source": [
    "def svm():\n",
    "    svc = SVC(kernel=\"rbf\")\n",
    "    model_fit(X_train_bow, y_train_bow, X_test_bow, y_test_bow, svc,\"SVM BOW\")\n",
    "    model_fit(X_train_tf_idf,y_train_tf_idf, X_test_tf_idf,  y_test_tf_idf,svc,\"SVM TF_IDF\")\n",
    "    model_fit(X_train_n_gram, y_train_n_gram, X_test_n_gram, y_test_n_gram,svc,\"SVM 2_GRAM\")\n",
    "    model_fit(X_train_3_gram, y_train_3_gram, X_test_3_gram, y_test_3_gram,svc,\"SVM 3_GRAM\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cada0a41",
   "metadata": {},
   "source": [
    "## naive_bayes\n",
    "  \n",
    "* **this is user defined function whice apply  naive_bayes model**  \n",
    "* **then calculate cross_validation for all three Transform**\n",
    "* **[ BOW, and TF-IDF, n-gram]**\n",
    "* **then calculate model_fit  for all three Transform [ BOW, and TF-IDF, n-gram]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "kpxqOd5mZgD5",
   "metadata": {
    "id": "kpxqOd5mZgD5"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "def naive_bayes():\n",
    "    nb = MultinomialNB()\n",
    "    model_fit(X_train_bow, y_train_bow,X_test_bow,y_test_bow,nb,\"Naïve Bayes BOW\")\n",
    "    model_fit(X_train_tf_idf,y_train_tf_idf, X_test_tf_idf,  y_test_tf_idf,nb,\"Naïve Bayes TF_IDF\")\n",
    "    model_fit(X_train_n_gram, y_train_n_gram, X_test_n_gram, y_test_n_gram,nb,\"Naïve Bayes N_GRAM\")\n",
    "    model_fit(X_train_3_gram, y_train_3_gram ,X_test_3_gram, y_test_3_gram,nb,\"Naïve Bayes 3_GRAM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523f4cb8",
   "metadata": {},
   "source": [
    "## knn\n",
    "* **this is user defined function whice apply  KNeighborsClassifier model  with k = 5**\n",
    "* **then calculate cross_validation for all three Transform**\n",
    "* **[ BOW, and TF-IDF, n-gram]**\n",
    "* **then calculate model_fit  for all three Transform [ BOW, and TF-IDF, n-gram]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90a74457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn():\n",
    "    k = 5  # Set the value of k for KNN\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    model_fit(X_train_bow, y_train_bow,X_test_bow,y_test_bow,knn,\"KNN BOW\")\n",
    "    model_fit(X_train_tf_idf,y_train_tf_idf, X_test_tf_idf,  y_test_tf_idf,knn,\"KNN TF_IDF\")\n",
    "    model_fit(X_train_n_gram, y_train_n_gram, X_test_n_gram, y_test_n_gram,knn,\"KNN 2_GRAM\")\n",
    "    model_fit(X_train_3_gram, y_train_3_gram ,X_test_3_gram, y_test_3_gram,knn,\"KNN 3_GRAM\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677987d3",
   "metadata": {},
   "source": [
    "## RandomForest\n",
    "* **this is user defined function whice apply  RandomForestClassifier model**\n",
    "* **then calculate cross_validation for all three Transform**\n",
    "* **[ BOW, and TF-IDF, n-gram]**\n",
    "* **then calculate model_fit  for all three Transform [ BOW, and TF-IDF, n-gram]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1d6d95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForest():\n",
    "    Forest_model = RandomForestClassifier(n_estimators = 200, criterion = 'entropy', random_state = 42)\n",
    "    model_fit(X_train_bow, y_train_bow,X_test_bow,y_test_bow,Forest_model,\"Random Forest BOW\")\n",
    "    model_fit(X_train_tf_idf,y_train_tf_idf, X_test_tf_idf,  y_test_tf_idf,Forest_model,\"Random Forest TF_IDF\")\n",
    "    model_fit(X_train_n_gram, y_train_n_gram, X_test_n_gram, y_test_n_gram,Forest_model,\"Random Forest 2_GRAM\")\n",
    "    model_fit(X_train_3_gram, y_train_3_gram ,X_test_3_gram, y_test_3_gram,Forest_model,\"Random Forest 3_GRAM\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ac53d5",
   "metadata": {},
   "source": [
    "## SGD\n",
    "* **this is user defined function whice apply  SGDClassifier model**\n",
    "* **then calculate cross_validation for all three Transform**\n",
    "* **[ BOW, and TF-IDF, n-gram]**\n",
    "* **then calculate model_fit  for all three Transform [ BOW, and TF-IDF, n-gram]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "813b9c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD():\n",
    "    sgd = SGDClassifier(random_state=42)\n",
    "    model_fit(X_train_bow, y_train_bow,X_test_bow,y_test_bow,sgd,\"SGD BOW\")\n",
    "    model_fit(X_train_tf_idf,y_train_tf_idf, X_test_tf_idf,  y_test_tf_idf,sgd,\"SGD TF_IDF\")\n",
    "    model_fit(X_train_n_gram, y_train_n_gram, X_test_n_gram, y_test_n_gram,sgd,\"SGD 2_GRAM\")\n",
    "    model_fit(X_train_3_gram, y_train_3_gram, X_test_3_gram, y_test_3_gram,sgd,\"SGD 3_GRAM\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46937025",
   "metadata": {},
   "source": [
    "## xgb_m\n",
    "\n",
    "* **this is user defined function whice apply  XGBClassifier model**\n",
    "* **then calculate cross_validation for all three Transform\\n**\",\n",
    "* **[ BOW, and TF-IDF, n-gram]**\n",
    "* **then calculate model_fit  for all three Transform [ BOW, and TF-IDF, n-gram]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8b190c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_m():\n",
    "    xgb_model = xgb.XGBClassifier(random_state=42)\n",
    "    model_fit(X_train_bow, y_train_bow,X_test_bow,y_test_bow,xgb_model,\"XGB BOW\")\n",
    "    model_fit(X_train_tf_idf,y_train_tf_idf, X_test_tf_idf,  y_test_tf_idf,xgb_model,\"XGB TF_IDF\")\n",
    "    model_fit(X_train_n_gram, y_train_n_gram, X_test_n_gram, y_test_n_gram,xgb_model,\"XGB 2_GRAM\")\n",
    "    model_fit(X_train_3_gram, y_train_3_gram, X_test_3_gram, y_test_3_gram,xgb_model,\"XGB 3_GRAM\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df03232",
   "metadata": {},
   "source": [
    "## Get 5 Books from Gutenberg Library \n",
    "\n",
    "\n",
    "*  **The Project Gutenberg EBook of The Adventures of Pinocchio, by C. Collodi--Pseudonym of Carlo Lorenzini**\n",
    "\n",
    "*  **The Project Gutenberg eBook of Peter Pan, by James M. Barrie**\n",
    "\n",
    "*  **The Project Gutenberg eBook of The Wonderful Wizard of Oz, by L. Frank Baum**\n",
    "\n",
    "*  **The Project Gutenberg eBook of Alice’s Adventures in Wonderland, by Lewis Carroll**\n",
    "\n",
    "*  **The Project Gutenberg eBook of Gulliver’s Travels, by Jonathan Swift**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73c202a9",
   "metadata": {
    "id": "73c202a9"
   },
   "outputs": [],
   "source": [
    "pinocchio = \"https://www.gutenberg.org/files/500/500-0.txt\"\n",
    "pan = \"https://www.gutenberg.org/files/16/16-0.txt\"\n",
    "oz = \"https://www.gutenberg.org/files/55/55-0.txt\"\n",
    "alice = \"https://www.gutenberg.org/files/11/11-0.txt\"\n",
    "gulliver = \"https://www.gutenberg.org/files/829/829-0.txt\"\n",
    "books_list = [pinocchio , pan , oz , alice , gulliver]\n",
    "author_list = [\"Carlo Collodi\", \"James M. Barrie\", \"L. Frank Baum\", \"Lewis Carroll\", \"Jonathan Swift\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afddd1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = five_books(books_list,author_list, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IyFOV5PkSCSt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IyFOV5PkSCSt",
    "outputId": "48601af9-f17d-4b9e-baa7-1e970084cf16"
   },
   "outputs": [],
   "source": [
    "df.author.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3db32b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "lzkiErbTozs0",
    "outputId": "9c82b29b-999c-4a3b-bf8c-ff4533420ca3"
   },
   "outputs": [],
   "source": [
    "df['char_count'] = df[\"partitions\"].apply(lambda x: sum(len(word) for word in str(x).split(\" \")))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wsrm3IOQoqPS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wsrm3IOQoqPS",
    "outputId": "39eefb78-1a9a-47c9-a612-2609eaf4e913",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "draw_chart(df,author_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1BUVT0FTo3o2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1BUVT0FTo3o2",
    "outputId": "4fe1b2c2-264e-441e-ac69-16105ecef5c5"
   },
   "outputs": [],
   "source": [
    "draw_chart_histogram(df,author_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ihDb87NLSU9C",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ihDb87NLSU9C",
    "outputId": "6ecf9edb-3308-4e8e-98e9-663a914dd18a"
   },
   "outputs": [],
   "source": [
    "Y = df.author\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "K29l4Lsfo-1H",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 586
    },
    "id": "K29l4Lsfo-1H",
    "outputId": "b22534b6-bdc1-4275-a3c3-c97c6c547da5"
   },
   "outputs": [],
   "source": [
    "common_words = get_top_n_words(df['partitions'], 20)\n",
    "df1 = pd.DataFrame(common_words, columns = ['word' , 'count'])\n",
    "df1.groupby('word').sum()['count'].sort_values(ascending=False).plot(kind='bar', title='Top 20 words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yVi2KHhHo7Du",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yVi2KHhHo7Du",
    "outputId": "0d07f725-d0a7-4390-ee39-ac904a530280"
   },
   "outputs": [],
   "source": [
    "a = bow(df)\n",
    "draw_TSNE(a[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb32c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tf_idf(df)\n",
    "draw_TSNE(t[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab187244",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = n_gram(df,2)\n",
    "draw_TSNE(g[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cface655",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = n_gram(df,3)\n",
    "draw_TSNE(g[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gddYQeKFQHhn",
   "metadata": {
    "id": "gddYQeKFQHhn"
   },
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "df.author = encoder.fit_transform(df.author)\n",
    "Y = df.author\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yVMB_WHdUaP6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yVMB_WHdUaP6",
    "outputId": "e42ca52f-f12f-4b5d-8cb8-218da66878c1"
   },
   "outputs": [],
   "source": [
    "X_train_bow, X_test_bow, y_train_bow, y_test_bow,hh = bow(df)\n",
    "X_train_tf_idf, X_test_tf_idf, y_train_tf_idf, y_test_tf_idf,tt = tf_idf(df)\n",
    "X_train_n_gram, X_test_n_gram, y_train_n_gram, y_test_n_gram,gg = n_gram(df,2)\n",
    "X_train_3_gram, X_test_3_gram, y_train_3_gram, y_test_3_gram,gg = n_gram(df,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8-AZ_2Whe14q",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "8-AZ_2Whe14q",
    "outputId": "8b6f8054-9923-4c2b-d7cb-fd4a36e370ba"
   },
   "outputs": [],
   "source": [
    "X_train_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LVtxiiG-Y7bU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LVtxiiG-Y7bU",
    "outputId": "7c23ceec-b4c2-4e72-dc36-30a7ee98996d",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "svm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e3d90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181d57e2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "knn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8fbd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03c4ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SGD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead510bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_m() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61cc3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = five_books(books_list,author_list, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddde6bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567109ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c610a464",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b7e13b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e4bfa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccb35ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fbce10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb85056",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbccce4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ceecc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daebbe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00182356",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a9ba11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310eeb4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
